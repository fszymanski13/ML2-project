{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 200)\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd().parent / \"data/r2.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to adress the outliers, quantile bucketization has been applied for chlorides, residual sugar and sulphates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_chlorides = list(df['chlorides'].quantile([0, 0.25, 0.5, 0.75, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "df['chlorides'] = pd.cut(df['chlorides'], bins=quantiles_chlorides, labels=labels, right=True, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1    356\n",
       "Q3    355\n",
       "Q2    347\n",
       "Q4    342\n",
       "Name: chlorides, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['chlorides'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_residual_sugar = list(df['residual sugar'].quantile([0, 0.25, 0.5, 0.75, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "df['residual sugar'] = pd.cut(df['residual sugar'], bins=quantiles_residual_sugar, labels=labels, right=True, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_sulphates = list(df['sulphates'].quantile([0, 0.25, 0.5, 0.75, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "df['sulphates'] = pd.cut(df['sulphates'], bins=quantiles_sulphates, labels=labels, right=True, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numeric features MinMax scaling has been applied, and categorical ones have been encoded using one hot encoder.\n",
    "Preprocessor has been fitted and transformed on the train sample, only transformed on the tast sample to avoid data leakge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['quality', 'id'], axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "cols_to_scale = X_train.columns.drop(['chlorides', 'residual sugar', 'sulphates'])\n",
    "cols_to_encode = ['chlorides', 'residual sugar', 'sulphates']\n",
    "\n",
    "numeric_transformer = MinMaxScaler()\n",
    "categoric_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), cols_to_scale),\n",
    "        ('cat', OneHotEncoder(), cols_to_encode)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "columns = cols_to_scale.tolist() + preprocessor.named_transformers_['cat'].get_feature_names_out(cols_to_encode).tolist()\n",
    "X_train = pd.DataFrame(X_train, columns=columns)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample, X_val, y_train_sample, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, 8 base regressors have been applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_MAPE</th>\n",
       "      <th>Val_MAPE</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.166873</td>\n",
       "      <td>0.090677</td>\n",
       "      <td>17.585576</td>\n",
       "      <td>19.54868</td>\n",
       "      <td>1.119756</td>\n",
       "      <td>1.258874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>0.44701</td>\n",
       "      <td>0.188573</td>\n",
       "      <td>13.274385</td>\n",
       "      <td>18.837877</td>\n",
       "      <td>0.912276</td>\n",
       "      <td>1.189181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.156318</td>\n",
       "      <td>0.284894</td>\n",
       "      <td>18.755004</td>\n",
       "      <td>0.021644</td>\n",
       "      <td>1.212586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging SVR</th>\n",
       "      <td>0.44708</td>\n",
       "      <td>0.193568</td>\n",
       "      <td>13.751856</td>\n",
       "      <td>18.723399</td>\n",
       "      <td>0.912218</td>\n",
       "      <td>1.185515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.972617</td>\n",
       "      <td>0.197541</td>\n",
       "      <td>3.043187</td>\n",
       "      <td>18.487144</td>\n",
       "      <td>0.203005</td>\n",
       "      <td>1.182591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.892246</td>\n",
       "      <td>0.242856</td>\n",
       "      <td>6.337504</td>\n",
       "      <td>17.75722</td>\n",
       "      <td>0.402702</td>\n",
       "      <td>1.148716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bagging Linear Regression</th>\n",
       "      <td>0.257351</td>\n",
       "      <td>0.242384</td>\n",
       "      <td>16.64001</td>\n",
       "      <td>17.588544</td>\n",
       "      <td>1.057206</td>\n",
       "      <td>1.149073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.252533</td>\n",
       "      <td>0.238291</td>\n",
       "      <td>16.569106</td>\n",
       "      <td>17.485227</td>\n",
       "      <td>1.06063</td>\n",
       "      <td>1.152173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Train_R2    Val_R2 Train_MAPE   Val_MAPE  \\\n",
       "Decision Tree              0.166873  0.090677  17.585576   19.54868   \n",
       "SVR                         0.44701  0.188573  13.274385  18.837877   \n",
       "XGBoost                    0.999689  0.156318   0.284894  18.755004   \n",
       "Bagging SVR                 0.44708  0.193568  13.751856  18.723399   \n",
       "LightGBM                   0.972617  0.197541   3.043187  18.487144   \n",
       "Random Forest              0.892246  0.242856   6.337504   17.75722   \n",
       "Bagging Linear Regression  0.257351  0.242384   16.64001  17.588544   \n",
       "Linear Regression          0.252533  0.238291  16.569106  17.485227   \n",
       "\n",
       "                          Train_RMSE  Val_RMSE  \n",
       "Decision Tree               1.119756  1.258874  \n",
       "SVR                         0.912276  1.189181  \n",
       "XGBoost                     0.021644  1.212586  \n",
       "Bagging SVR                 0.912218  1.185515  \n",
       "LightGBM                    0.203005  1.182591  \n",
       "Random Forest               0.402702  1.148716  \n",
       "Bagging Linear Regression   1.057206  1.149073  \n",
       "Linear Regression            1.06063  1.152173  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Empty DataFrame to store results\n",
    "results_df = pd.DataFrame(index=['Linear Regression', 'Bagging Linear Regression', 'SVR', 'Bagging SVR', 'Decision Tree', 'Random Forest', 'XGBoost', 'LightGBM'],\n",
    "                           columns=['Train_R2', 'Val_R2', 'Train_MAPE', 'Val_MAPE', 'Train_RMSE', 'Val_RMSE'])\n",
    "\n",
    "base_reg_lr = LinearRegression()\n",
    "base_reg_svr = SVR()\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Bagging Linear Regression': BaggingRegressor(base_reg_lr, n_estimators=30, random_state=123),\n",
    "    'SVR': SVR(),\n",
    "    'Bagging SVR': BaggingRegressor(base_reg_svr, n_estimators=30, random_state=123),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=123, max_depth=3, min_samples_leaf=0.13),\n",
    "    'Random Forest': RandomForestRegressor(random_state=123),\n",
    "    'XGBoost': XGBRegressor(random_state=123),\n",
    "    'LightGBM': LGBMRegressor(random_state=123)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model on the training set\n",
    "    model.fit(X_train_sample, y_train_sample)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    train_predictions = model.predict(X_train_sample)\n",
    "    train_r2 = r2_score(y_train_sample, train_predictions)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_sample, train_predictions))\n",
    "    train_mape = mean_absolute_percentage_error(y_train_sample, train_predictions)\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    val_predictions = model.predict(X_val)\n",
    "    val_r2 = r2_score(y_val, val_predictions)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "    val_mape = mean_absolute_percentage_error(y_val, val_predictions)\n",
    "    \n",
    "    results_df.loc[model_name] = [train_r2, val_r2, train_mape, val_mape, train_rmse, val_rmse]\n",
    "\n",
    "display(results_df.sort_values(by='Val_MAPE', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, voting regressor with 5 base models, that initially gave the best results was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Train_MAPE</th>\n",
       "      <th>Val_MAPE</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Val_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Voting Regressor</th>\n",
       "      <td>0.855406</td>\n",
       "      <td>0.211744</td>\n",
       "      <td>7.285072</td>\n",
       "      <td>18.239054</td>\n",
       "      <td>0.46649</td>\n",
       "      <td>1.172079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Train_R2    Val_R2 Train_MAPE   Val_MAPE Train_RMSE  \\\n",
       "Voting Regressor  0.855406  0.211744   7.285072  18.239054    0.46649   \n",
       "\n",
       "                  Val_RMSE  \n",
       "Voting Regressor  1.172079  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(index=['Voting Regressor'],\n",
    "                           columns=['Train_R2', 'Val_R2', 'Train_MAPE', 'Val_MAPE', 'Train_RMSE', 'Val_RMSE'])\n",
    "\n",
    "base_reg_lr = LinearRegression()\n",
    "base_reg_svr = SVR()\n",
    "\n",
    "models = [\n",
    "    ('Bagging Linear Regression', BaggingRegressor(base_reg_lr, n_estimators=30, random_state=123)),\n",
    "    ('Bagging SVR', BaggingRegressor(base_reg_svr, n_estimators=30, random_state=123)),\n",
    "    ('Decision Tree', DecisionTreeRegressor(random_state=123)),\n",
    "    ('Random Forest', RandomForestRegressor(random_state=123)),\n",
    "    ('XGBoost', XGBRegressor(random_state=123))\n",
    "]\n",
    "\n",
    "voting_regressor = VotingRegressor(models)\n",
    "\n",
    "# Fit the model on the training set\n",
    "voting_regressor.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_predictions = voting_regressor.predict(X_train_sample)\n",
    "train_r2 = r2_score(y_train_sample, train_predictions)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_sample, train_predictions))\n",
    "train_mape = mean_absolute_percentage_error(y_train_sample, train_predictions)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_predictions = voting_regressor.predict(X_val)\n",
    "val_r2 = r2_score(y_val, val_predictions)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "val_mape = mean_absolute_percentage_error(y_val, val_predictions)\n",
    "\n",
    "results_df.loc['Voting Regressor'] = [train_r2, val_r2, train_mape, val_mape, train_rmse, val_rmse]\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, stacking regressor with Linear Regression meta estimator has been fitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Test_R2</th>\n",
       "      <th>Train_MAPE</th>\n",
       "      <th>Test_MAPE</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Test_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stacking Regressor</th>\n",
       "      <td>0.698237</td>\n",
       "      <td>0.25451</td>\n",
       "      <td>10.580024</td>\n",
       "      <td>17.603045</td>\n",
       "      <td>0.673909</td>\n",
       "      <td>1.139841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Train_R2  Test_R2 Train_MAPE  Test_MAPE Train_RMSE  \\\n",
       "Stacking Regressor  0.698237  0.25451  10.580024  17.603045   0.673909   \n",
       "\n",
       "                   Test_RMSE  \n",
       "Stacking Regressor  1.139841  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(index=['Stacking Regressor'],\n",
    "                           columns=['Train_R2', 'Test_R2', 'Train_MAPE', 'Test_MAPE', 'Train_RMSE', 'Test_RMSE'])\n",
    "\n",
    "base_reg_lr = LinearRegression()\n",
    "base_reg_svr = SVR()\n",
    "\n",
    "base_models = [\n",
    "    ('Bagging Linear Regression', BaggingRegressor(base_reg_lr, n_estimators=30, random_state=123)),\n",
    "    ('Bagging SVR', BaggingRegressor(base_reg_svr, n_estimators=30, random_state=123)),\n",
    "    ('Decision Tree', DecisionTreeRegressor(random_state=123)),\n",
    "    ('Random Forest', RandomForestRegressor(random_state=123)),\n",
    "    ('XGBoost', XGBRegressor(random_state=123))\n",
    "]\n",
    "\n",
    "stacking_regressor = StackingRegressor(estimators=base_models, final_estimator=LinearRegression())\n",
    "\n",
    "# Fit the model on the training set\n",
    "stacking_regressor.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_predictions = stacking_regressor.predict(X_train_sample)\n",
    "train_r2 = r2_score(y_train_sample, train_predictions)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train_sample, train_predictions))\n",
    "train_mape = mean_absolute_percentage_error(y_train_sample, train_predictions)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "test_predictions = stacking_regressor.predict(X_val)\n",
    "test_r2 = r2_score(y_val, test_predictions)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_val, test_predictions))\n",
    "test_mape = mean_absolute_percentage_error(y_val, test_predictions)\n",
    "\n",
    "results_df.loc['Stacking Regressor'] = [train_r2, test_r2, train_mape, test_mape, train_rmse, test_rmse]\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1120, 30)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the initial performance of different models and configurations we processed with Sequental Feature Seection with Random Forest Regressor. The algorithm selected the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSS_rf = SequentialFeatureSelector(\n",
    "#     RandomForestRegressor(random_state=123),\n",
    "#     k_features=(1,30),\n",
    "#     forward=True,\n",
    "#     verbose=2,\n",
    "#     cv=5,\n",
    "#     scoring='neg_mean_absolute_percentage_error',\n",
    "#     n_jobs=-1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(FSS_rf.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names = ['alcohol',\n",
    " 'citric acid',\n",
    " 'feat01',\n",
    " 'feat04',\n",
    " 'feat07',\n",
    " 'free sulfur dioxide',\n",
    " 'pH',\n",
    " 'total sulfur dioxide',\n",
    " 'volatile acidity',\n",
    " 'chlorides_Q1',\n",
    " 'chlorides_Q2',\n",
    " 'chlorides_Q3',\n",
    " 'chlorides_Q4',\n",
    " 'residual sugar_Q1',\n",
    " 'residual sugar_Q2',\n",
    " 'residual sugar_Q3',\n",
    " 'residual sugar_Q4',\n",
    " 'sulphates_Q1',\n",
    " 'sulphates_Q2',\n",
    " 'sulphates_Q3',\n",
    " 'sulphates_Q4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[selected_feature_names]\n",
    "X_test = X_test[selected_feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to fine tune separately each model used in both stacking and voting regressors. For this purpose, Randomized search algorithm with cross-validation on 5 folds were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 80 is smaller than n_iter=100. Running 80 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 20, 'max_samples': 0.5, 'max_features': 1.0}\n",
      "RMSE: RandomizedSearchCV 1.0510461246277991\n",
      "MAPE: RandomizedSearchCV 15.407364490220651\n",
      "R2: RandomizedSearchCV 0.16162969028701635\n"
     ]
    }
   ],
   "source": [
    "# Base Linear Regression model\n",
    "base_model = LinearRegression()\n",
    "\n",
    "# Bagging Regressor with Linear Regression as base estimator\n",
    "bagging_reg = BaggingRegressor(base_model, random_state=123)\n",
    "\n",
    "# Hyperparameter grid for Bagging Regressor\n",
    "params = {\n",
    "    'n_estimators': [10, 20, 30, 40, 50],\n",
    "    'max_samples': [0.5, 0.7, 0.9, 1.0],\n",
    "    'max_features': [0.5, 0.7, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning\n",
    "random_search_bagging = RandomizedSearchCV(\n",
    "    estimator=bagging_reg,\n",
    "    param_distributions=params,\n",
    "    n_iter=100,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "random_search_bagging.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search_bagging.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = random_search_bagging.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"RMSE:\", random_search_bagging.__class__.__name__, rmse)\n",
    "print(\"MAPE:\", random_search_bagging.__class__.__name__, mape)\n",
    "print(\"R2:\", random_search_bagging.__class__.__name__, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_bagging_lr = {'n_estimators': 20, 'max_samples': 0.5, 'max_features': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 80 is smaller than n_iter=100. Running 80 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Best Hyperparameters for SVR: {'kernel': 'linear', 'gamma': 'scale', 'epsilon': 0.1, 'C': 1}\n",
      "Best Hyperparameters for Bagging Regressor: {'n_estimators': 20, 'max_samples': 1.0, 'max_features': 1.0}\n",
      "RMSE: RandomizedSearchCV 1.050015568641571\n",
      "MAPE: RandomizedSearchCV 15.385464547488878\n",
      "R2: RandomizedSearchCV 0.16327293685643185\n"
     ]
    }
   ],
   "source": [
    "base_model = SVR()\n",
    "\n",
    "# Hyperparameter grid for svr\n",
    "params_svr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'epsilon': [0.1, 0.2, 0.5],\n",
    "}\n",
    "\n",
    "# Hyperparameter grid for Bagging Regressor\n",
    "params_bagging_regressor = {\n",
    "    'n_estimators': [10, 20, 30, 40, 50],\n",
    "    'max_samples': [0.5, 0.7, 0.9, 1.0],\n",
    "    'max_features': [0.5, 0.7, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning for SVR\n",
    "random_search_svr = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=params_svr,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',  \n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "random_search_svr.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters for SVR\n",
    "best_params_svr = random_search_svr.best_params_\n",
    "\n",
    "# Base SVR model with optimized hyperparameters\n",
    "base_model_optimized = SVR(**best_params_svr)\n",
    "\n",
    "bagging_reg = BaggingRegressor(base_model_optimized, random_state=123)\n",
    "random_search_bagging = RandomizedSearchCV(\n",
    "    estimator=bagging_reg,\n",
    "    param_distributions=params_bagging_regressor,\n",
    "    n_iter=100,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "random_search_bagging.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for SVR:\", best_params_svr)\n",
    "print(\"Best Hyperparameters for Bagging Regressor:\", random_search_bagging.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = random_search_bagging.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"RMSE:\", random_search_bagging.__class__.__name__, rmse)\n",
    "print(\"MAPE:\", random_search_bagging.__class__.__name__, mape)\n",
    "print(\"R2:\", random_search_bagging.__class__.__name__, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_svr = {'kernel': 'linear', 'gamma': 'scale', 'epsilon': 0.1, 'C': 1}\n",
    "best_params_bagging_svr = {'n_estimators': 20, 'max_samples': 1.0, 'max_features': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 270 is smaller than n_iter=500. Running 270 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Decision Tree Regressor: {'splitter': 'random', 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 5}\n",
      "RMSE: RandomizedSearchCV 1.1085302055581054\n",
      "MAPE: RandomizedSearchCV 16.46987022805065\n",
      "R2: RandomizedSearchCV 0.06741719856247586\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeRegressor()\n",
    "\n",
    "params = {\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None], \n",
    "    'splitter': ['best', 'random'],\n",
    "}\n",
    "\n",
    "\n",
    "random_search_decision_tree = RandomizedSearchCV(\n",
    "    estimator=decision_tree,\n",
    "    param_distributions=params,\n",
    "    n_iter=500,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "random_search_decision_tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for Decision Tree Regressor:\", random_search_decision_tree.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = random_search_decision_tree.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "\n",
    "print(\"RMSE:\", random_search_decision_tree.__class__.__name__, rmse)\n",
    "print(\"MAPE:\", random_search_decision_tree.__class__.__name__, mape)\n",
    "print(\"R2:\", random_search_decision_tree.__class__.__name__, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_decision_tree = {'splitter': 'best', 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Hyperparameters for Random Forest: {'n_estimators': 200, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 5, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': False}\n",
      "RMSE: RandomizedSearchCV 1.01258485918395\n",
      "MAPE: RandomizedSearchCV 15.040934854435225\n",
      "R2: RandomizedSearchCV 0.22186455624615142\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.1, 0.2],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "\n",
    "random_search_random_forest = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "random_search_random_forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for Random Forest:\", random_search_random_forest.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = random_search_random_forest.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"RMSE:\", random_search_random_forest.__class__.__name__, rmse)\n",
    "print(\"MAPE:\", random_search_random_forest.__class__.__name__, mape)\n",
    "print(\"R2:\", random_search_random_forest.__class__.__name__, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf = {'n_estimators': 200, 'min_weight_fraction_leaf': 0.0, 'min_samples_split': 5, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best Hyperparameters for XGBoost: {'subsample': 0.5, 'reg_lambda': 1.0, 'reg_alpha': 0.1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.5}\n",
      "RMSE: RandomizedSearchCV 1.024684619324475\n",
      "MAPE: RandomizedSearchCV 15.295918861707545\n",
      "R2: RandomizedSearchCV 0.20315697762011253\n"
     ]
    }
   ],
   "source": [
    "base_model = XGBRegressor()\n",
    "\n",
    "params_xgboost = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 10, 15],\n",
    "    'subsample': [0.5, 0.7, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.9, 1.0],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda': [0.0, 0.1, 0.5, 1.0],\n",
    "    'min_child_weight': [0, 1, 2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "random_search_xgboost = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=params_xgboost,\n",
    "    n_iter=100,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "random_search_xgboost.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters for XGBoost:\", random_search_xgboost.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "predictions = random_search_xgboost.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"RMSE:\", random_search_xgboost.__class__.__name__, rmse)\n",
    "print(\"MAPE:\", random_search_xgboost.__class__.__name__, mape)\n",
    "print(\"R2:\", random_search_xgboost.__class__.__name__, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = {'subsample': 0.5, 'reg_lambda': 1.0, 'reg_alpha': 0.1, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the hyperparameter for each model tuned, time to check if we managed to improve results of base models. Let's do it in a proper way, defining a pipeline for for transformation and modelling and validate the results using 5 fold crossvalidation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stacking Regressor</th>\n",
       "      <td>1.062143</td>\n",
       "      <td>16.528961</td>\n",
       "      <td>0.248389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        RMSE       MAPE        R2\n",
       "Stacking Regressor  1.062143  16.528961  0.248389"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df.drop(['quality', 'id', 'density', 'feat02', 'feat03', 'feat05', 'feat06', 'feat08', 'feat09', 'feat10', 'fixed acidity'], axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "cols_to_scale = X.columns.drop(['chlorides', 'residual sugar', 'sulphates'])\n",
    "\n",
    "cols_to_encode = ['chlorides', 'residual sugar', 'sulphates']\n",
    "\n",
    "numeric_transformer = MinMaxScaler()\n",
    "categoric_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, cols_to_scale),\n",
    "        ('cat', categoric_transformer, cols_to_encode)\n",
    "    ]\n",
    ")\n",
    "\n",
    "base_reg_lr = LinearRegression()\n",
    "base_reg_svr = SVR(**best_params_svr)\n",
    "\n",
    "base_models = [\n",
    "    ('Bagging Linear Regression', BaggingRegressor(base_reg_lr, **best_params_bagging_lr, random_state=123)),\n",
    "    ('Bagging SVR', BaggingRegressor(base_reg_svr, **best_params_bagging_svr, random_state=123)),\n",
    "    ('Decision Tree', DecisionTreeRegressor(**best_params_decision_tree, random_state=123)),\n",
    "    ('Random Forest', RandomForestRegressor(**best_params_rf, random_state=123)),\n",
    "    ('XGBoost', XGBRegressor(**best_params_xgb, random_state=123))\n",
    "]\n",
    "\n",
    "stacking_regressor = StackingRegressor(estimators=base_models, final_estimator=base_reg_lr)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', stacking_regressor)\n",
    "    ] \n",
    ")\n",
    "\n",
    "def mape_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return mean_absolute_percentage_error(y, y_pred)\n",
    "\n",
    "cv_rmse = cross_val_score(pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "cv_mape = cross_val_score(pipeline, X, y, cv=5, scoring=mape_scorer)\n",
    "cv_r2 = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "\n",
    "resutls_df2 = pd.DataFrame(index=['Stacking Regressor'],\n",
    "                            columns=['RMSE', 'MAPE', 'R2'])\n",
    "\n",
    "resutls_df2.loc['Stacking Regressor'] = [-cv_rmse.mean(), cv_mape.mean(), cv_r2.mean()]\n",
    "display(resutls_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stacking Regressor</th>\n",
       "      <td>1.062143</td>\n",
       "      <td>16.528961</td>\n",
       "      <td>0.248389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting Regressor</th>\n",
       "      <td>1.08101</td>\n",
       "      <td>16.854866</td>\n",
       "      <td>0.221863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        RMSE       MAPE        R2\n",
       "Stacking Regressor  1.062143  16.528961  0.248389\n",
       "Voting Regressor     1.08101  16.854866  0.221863"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df.drop(['quality', 'id', 'density', 'feat02', 'feat03', 'feat05', 'feat06', 'feat08', 'feat09', 'feat10', 'fixed acidity'], axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "cols_to_scale = X.columns.drop(['chlorides', 'residual sugar', 'sulphates'])\n",
    "\n",
    "cols_to_encode = ['chlorides', 'residual sugar', 'sulphates']\n",
    "\n",
    "numeric_transformer = MinMaxScaler()\n",
    "categoric_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, cols_to_scale),\n",
    "        ('cat', categoric_transformer, cols_to_encode)\n",
    "    ]\n",
    ")\n",
    "\n",
    "base_reg_lr = LinearRegression()\n",
    "base_reg_svr = SVR(**best_params_svr)\n",
    "\n",
    "models = [\n",
    "    ('Bagging Linear Regression', BaggingRegressor(base_reg_lr, **best_params_bagging_lr, random_state=123)),\n",
    "    ('Bagging SVR', BaggingRegressor(base_reg_svr, **best_params_bagging_svr, random_state=123)),\n",
    "    ('Decision Tree', DecisionTreeRegressor(**best_params_decision_tree, random_state=123)),\n",
    "    ('Random Forest', RandomForestRegressor(**best_params_rf, random_state=123)),\n",
    "    ('XGBoost', XGBRegressor(**best_params_xgb, random_state=123))\n",
    "]\n",
    "\n",
    "voting_regressor = VotingRegressor(models)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', voting_regressor)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def mape_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return mean_absolute_percentage_error(y, y_pred)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_rmse = cross_val_score(pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "cv_mape = cross_val_score(pipeline, X, y, cv=5, scoring=mape_scorer)\n",
    "cv_r2 = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "\n",
    "resutls_df2.loc['Voting Regressor'] = [-cv_rmse.mean(), cv_mape.mean(), cv_r2.mean()]\n",
    "display(resutls_df2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And additionally, Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stacking Regressor</th>\n",
       "      <td>1.062143</td>\n",
       "      <td>16.528961</td>\n",
       "      <td>0.248389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting Regressor</th>\n",
       "      <td>1.08101</td>\n",
       "      <td>16.854866</td>\n",
       "      <td>0.221863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>1.062052</td>\n",
       "      <td>16.545573</td>\n",
       "      <td>0.248867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        RMSE       MAPE        R2\n",
       "Stacking Regressor  1.062143  16.528961  0.248389\n",
       "Voting Regressor     1.08101  16.854866  0.221863\n",
       "Random Forest       1.062052  16.545573  0.248867"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df.drop(['quality', 'id', 'density', 'feat02', 'feat03', 'feat05', 'feat06', 'feat08', 'feat09', 'feat10', 'fixed acidity'], axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "cols_to_scale = X.columns.drop(['chlorides', 'residual sugar', 'sulphates'])\n",
    "\n",
    "cols_to_encode = ['chlorides', 'residual sugar', 'sulphates']\n",
    "\n",
    "numeric_transformer = MinMaxScaler()\n",
    "categoric_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, cols_to_scale),\n",
    "        ('cat', categoric_transformer, cols_to_encode)\n",
    "    ]\n",
    ")\n",
    "\n",
    "rf = RandomForestRegressor(**best_params_rf, random_state=123)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', rf)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def mape_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return mean_absolute_percentage_error(y, y_pred)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_rmse = cross_val_score(pipeline, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "cv_mape = cross_val_score(pipeline, X, y, cv=5, scoring=mape_scorer)\n",
    "cv_r2 = cross_val_score(pipeline, X, y, cv=5, scoring='r2')\n",
    "\n",
    "resutls_df2.loc['Random Forest'] = [-cv_rmse.mean(), cv_mape.mean(), cv_r2.mean()]\n",
    "display(resutls_df2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the very end, we can check the feature importance in our model. It seems like features 04 and 01, added by the Proffessor, player ver significant role in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4kAAAIjCAYAAACu3l5pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEyklEQVR4nO3deViV1f7//9dmRmRDjkiakKKp4FzmFJYWZmlqJ9IspUwbJG3A07G0REsaHLPUrJOa9s2yQTt61DQ1C2eTyiHFET+lWTkgWaLs9fujH+u0BQ2U7Q55Pq5rX4d77XWv+70W95Hr1dqDwxhjBAAAAACAJB9vFwAAAAAA+PsgJAIAAAAALEIiAAAAAMAiJAIAAAAALEIiAAAAAMAiJAIAAAAALEIiAAAAAMAiJAIAAAAALEIiAAAAAMAiJAIAAAAALEIiAOAvORyOIj1WrFjhtVpeeOGFvzx3+vTpZz3/X//6l0fqXbVqlYYPH66jR496ZHxPS0pKUvny5b1dxnk7ceKEhg8fflHuTQC4VPh5uwAAwN/fzJkz3Y7ffvttLVmypEB7vXr1Lko9N954o3r37u3W1qRJkyKfP2LECEVHR7u1xcbGlkhtZ1q1apVSU1OVlJSk8PBwj1wDZ3fixAmlpqZKktq1a+fdYgCglCAkAgD+0t133+12vGbNGi1ZsqRA+8VSp06dC7r2zTffrObNm5dgRRffr7/+qpCQEG+X8bflcrmUm5vr7TIAoFTi5aYAgBLx66+/6oknnlCNGjUUGBiounXravTo0TLGuPVzOBxKTk7WO++8o7p16yooKEjNmjXTypUri3W93377Tb///ntJTsFauHCh2rZtq5CQEIWGhuqWW27Rli1b3Pp88803SkpK0pVXXqmgoCBFRETovvvu0y+//GL7DB8+XIMHD5YkRUdH25e27t27V3v37pXD4dD06dMLXN/hcGj48OFu4zgcDm3dulV33XWXLrvsMrVp08Y+P2vWLDVr1kzBwcGqUKGCevToof3797uNmZmZqdtvv10REREKCgpS9erV1aNHDx07dqzY6xMVFaVbb71VK1asUPPmzRUcHKy4uDj7ks6PPvpIcXFx9ne7adMmt/PzX8K6e/duJSQkKCQkRJGRkRoxYkSB++V87qsGDRooMDBQU6ZMUeXKlSVJqampdv3z17Yov8M/r//OnTvtjnBYWJjuvfdenThxosD6zJo1S9dcc43KlSunyy67TNddd50+/fRTtz5FuccAwFvYSQQAXDBjjLp06aLly5erb9++aty4sRYvXqzBgwfr+++/17hx49z6f/7553rvvfc0cOBABQYGatKkSerYsaPWrVtXpJd9Tp8+XZMmTZIxRvXq1dPQoUN11113FbneY8eO6eeff3Zrq1SpkqQ/Xlrbp08fJSQk6MUXX9SJEyc0efJktWnTRps2bVJUVJQkacmSJdq9e7fuvfdeRUREaMuWLZo6daq2bNmiNWvWyOFwqHv37tqxY4feffddjRs3zl6jcuXK+umnn4pcb7477rhDMTExGjVqlA1Jzz//vIYNG6bExETdf//9+umnnzRx4kRdd9112rRpk8LDw5Wbm6uEhASdPHlSjzzyiCIiIvT9999r/vz5Onr0qMLCwopdy86dO3XXXXfpgQce0N13363Ro0erc+fOmjJlip566ik9/PDDkqS0tDQlJiZq+/bt8vH533+bzsvLU8eOHXXttdfqpZde0qJFi/Tss8/q9OnTGjFihKTi31fLli3T+++/r+TkZFWqVEmNGjXS5MmT9dBDD6lbt27q3r27JKlhw4aSivY7/LPExERFR0crLS1NX331ld58801VqVJFL774ou2Tmpqq4cOHq1WrVhoxYoQCAgK0du1aLVu2TDfddJOkot9jAOA1BgCAYhowYID585+QuXPnGknmueeec+v3j3/8wzgcDrNz507bJslIMhs2bLBt+/btM0FBQaZbt25/ee1WrVqZ8ePHm3nz5pnJkyeb2NhYI8lMmjTpL8+dNm2avf6ZD2OMOX78uAkPDzf9+vVzO+/gwYMmLCzMrf3EiRMFxn/33XeNJLNy5Urb9vLLLxtJZs+ePW599+zZYySZadOmFRhHknn22Wft8bPPPmskmZ49e7r127t3r/H19TXPP/+8W/u3335r/Pz8bPumTZuMJDNnzpyzL85Z9OnTx4SEhLi11axZ00gyq1atsm2LFy82kkxwcLDZt2+fbX/99deNJLN8+XK3MSWZRx55xLa5XC5zyy23mICAAPPTTz8ZY4p/X/n4+JgtW7a49f3pp58KrGe+ov4O89f/vvvuc+vbrVs3U7FiRXucmZlpfHx8TLdu3UxeXp5bX5fLZYwp3j0GAN7Cy00BABfsv//9r3x9fTVw4EC39ieeeELGGC1cuNCtvWXLlmrWrJk9vuKKK3Tbbbdp8eLFysvLO+e10tPTNWjQIHXp0kUPPvigNm7cqNjYWD311FP67bffilTva6+9piVLlrg9pD92lo4ePaqePXvq559/tg9fX1+1aNFCy5cvt2MEBwfbn3///Xf9/PPPuvbaayVJX331VZHqKK4HH3zQ7fijjz6Sy+VSYmKiW70RERGKiYmx9ebvFC5evLjQl0eej/r166tly5b2uEWLFpKkG264QVdccUWB9t27dxcYIzk52f6c/3LR3NxcLV26VFLx76v4+HjVr1+/yHMo7u/wzPVv27atfvnlF2VnZ0uS5s6dK5fLpWeeecZt1zR/flLx7jEA8BZebgoAuGD79u1TZGSkQkND3drzP+103759bu0xMTEFxqhTp45OnDihn376SREREUW+dkBAgJKTk21g/PN79c7mmmuuKfSDazIzMyX9EXQK43Q67c+HDx9WamqqZs+erUOHDrn1O5/3+RXFmZ/ImpmZKWNMoespSf7+/va8xx9/XGPHjtU777yjtm3bqkuXLrr77rvP66WmktyCoPS/IFqjRo1C248cOeLW7uPjoyuvvNKtrU6dOpKkvXv3Sir+fXXm+vyV4v4Oz5zzZZddJumPuTmdTu3atUs+Pj7nDKrFuccAwFsIiQCAUi8/mBw+fPiCxnG5XJL+eM9YYUHVz+9/fzYTExO1atUqDR48WI0bN1b58uXlcrnUsWNHO865nPl+t3zn2kn9885Xfr0Oh0MLFy6Ur69vgf5//n7DMWPGKCkpSfPmzdOnn36qgQMHKi0tTWvWrFH16tX/st4zFXa9c7WbMz5oxhPOXJ+/UtzfYUnMrTj3GAB4C/8SAQAuWM2aNbV06VIdP37cbdfnu+++s8//Wf5uyp/t2LFD5cqVs59GWRz5L2U8n3P/rFatWpKkKlWqqEOHDmftd+TIEX322WdKTU3VM888Y9sLm9fZwmD+LtTRo0fd2s/cHfureo0xio6Otrtw5xIXF6e4uDgNHTpUq1atUuvWrTVlyhQ999xzRb5mSXG5XNq9e7db3Tt27JAk+8Etxb2vCnO29S/O77CoatWqJZfLpa1bt6px48Zn7SP99T0GAN7EexIBABesU6dOysvL06uvvurWPm7cODkcDt18881u7atXr3Z7z9f+/fs1b9483XTTTWfdrZFU6CeCHj9+XOPHj1elSpXc3ud4PhISEuR0OjVq1CidOnXqrNfPr/HMHaTx48cXOCf/uwzPDINOp1OVKlUq8NUfkyZNKnK93bt3l6+vr1JTUwvUYoyxX+WQnZ2t06dPuz0fFxcnHx8fnTx5ssjXK2l/vl+MMXr11Vfl7++v9u3bSyr+fVWYcuXKSSq4/sX5HRZV165d5ePjoxEjRhTYicy/TlHvMQDwJnYSAQAXrHPnzrr++uv19NNPa+/evWrUqJE+/fRTzZs3T48++qjdPckXGxurhIQEt6/AkP74+oBzee211zR37lx17txZV1xxhQ4cOKC33npLWVlZmjlzpgICAi5oHk6nU5MnT9Y999yjpk2bqkePHqpcubKysrK0YMECtW7dWq+++qqcTqeuu+46vfTSSzp16pQuv/xyffrpp9qzZ0+BMfOD69NPP60ePXrI399fnTt3VkhIiO6//3698MILuv/++9W8eXOtXLnS7qYVRa1atfTcc89pyJAh2rt3r7p27arQ0FDt2bNHH3/8sfr376+UlBQtW7ZMycnJuuOOO1SnTh2dPn1aM2fOlK+vr26//fYLWrPzFRQUpEWLFqlPnz5q0aKFFi5cqAULFuipp56yO8LFva8KExwcrPr16+u9995TnTp1VKFCBcXGxio2NrbIv8Oiql27tp5++mmNHDlSbdu2Vffu3RUYGKj169crMjJSaWlpRb7HAMCrvPOhqgCA0uzMr8Aw5o+P9n/sscdMZGSk8ff3NzExMebll1+2H/2fT5IZMGCAmTVrlomJiTGBgYGmSZMmbl+RcDaffvqpufHGG01ERITx9/c34eHh5qabbjKfffZZkerO/wqM9evXn7Pf8uXLTUJCggkLCzNBQUGmVq1aJikpye1rO/7v//7PdOvWzYSHh5uwsDBzxx13mB9++KHQr1sYOXKkufzyy42Pj4/b12GcOHHC9O3b14SFhZnQ0FCTmJhoDh06dNavwMj/aogzffjhh6ZNmzYmJCTEhISEmKuuusoMGDDAbN++3RhjzO7du819991natWqZYKCgkyFChXM9ddfb5YuXfqXa3a2r8C45ZZbCvTN/93+Wf5Xfbz88ssFxty1a5e56aabTLly5UzVqlXNs88+W+CrI4p7XxVm1apVplmzZiYgIMBtbYv6Ozzb+uffT2d+vclbb71lmjRpYgIDA81ll11m4uPjzZIlS9z6FOUeAwBvcRhzEd5JDgDA/8/hcGjAgAHslpRhSUlJ+uCDD5STk+PtUgAAheA9iQAAAAAAi5AIAAAAALAIiQAAAAAAi/ckAgAAAAAsdhIBAAAAABYhEQAAAABg+Xm7AJQ8l8ulH374QaGhoXI4HN4uBwAAAICXGGN0/PhxRUZGysenaHuEhMRL0A8//KAaNWp4uwwAAAAAfxP79+9X9erVi9SXkHgJCg0NlfTHjeB0Or1cDQAAAABvyc7OVo0aNWxGKApC4iUo/yWmTqeTkAgAAACgWG9D44NrAAAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYPl5uwB4Tu+lm+UfUt7bZQAAAABlxpyEht4u4YKxkwgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJAAAAAACLkAgAAAAAsAiJkowx6t+/vypUqCCHw6GMjAxvlwQAAAAAXkFIlLRo0SJNnz5d8+fP14EDBxQbG3vBYyYlJalr164F2g8fPqxevXrJ6XQqPDxcffv2VU5OTqFj7Ny5U6GhoQoPD7/gegAAAACgKAiJknbt2qVq1aqpVatWioiIkJ+fn8eu1atXL23ZskVLlizR/PnztXLlSvXv379Av1OnTqlnz55q27atx2oBAAAAgDOV+ZCYlJSkRx55RFlZWXI4HIqKipLL5VJaWpqio6MVHBysRo0a6YMPPrDn5OXlqW/fvvb5unXrasKECfb54cOHa8aMGZo3b54cDoccDodWrFihbdu2adGiRXrzzTfVokULtWnTRhMnTtTs2bP1ww8/uNU1dOhQXXXVVUpMTLxoawEAAAAAntsyKyUmTJigWrVqaerUqVq/fr18fX2VlpamWbNmacqUKYqJidHKlSt19913q3LlyoqPj5fL5VL16tU1Z84cVaxYUatWrVL//v1VrVo1JSYmKiUlRdu2bVN2dramTZsmSapQoYJmzZql8PBwNW/e3F6/Q4cO8vHx0dq1a9WtWzdJ0rJlyzRnzhxlZGToo48++ss5nDx5UidPnrTH2dnZJbxKAAAAAMqKMh8Sw8LCFBoaKl9fX0VEROjkyZMaNWqUli5dqpYtW0qSrrzySn355Zd6/fXXFR8fL39/f6WmptoxoqOjtXr1ar3//vtKTExU+fLlFRwcrJMnTyoiIsL2O3jwoKpUqeJ2fT8/P1WoUEEHDx6UJP3yyy9KSkrSrFmz5HQ6izSHtLQ0t3oAAAAA4HyV+ZB4pp07d+rEiRO68cYb3dpzc3PVpEkTe/zaa6/prbfeUlZWln777Tfl5uaqcePGF3z9fv366a677tJ1111X5HOGDBmixx9/3B5nZ2erRo0aF1wLAAAAgLKHkHiG/E8aXbBggS6//HK35wIDAyVJs2fPVkpKisaMGaOWLVsqNDRUL7/8stauXXvOsSMiInTo0CG3ttOnT+vw4cN2x3HZsmX65JNPNHr0aEl/fD2Hy+WSn5+fpk6dqvvuu6/AuIGBgbY2AAAAALgQhMQz1K9fX4GBgcrKylJ8fHyhfdLT09WqVSs9/PDDtm3Xrl1ufQICApSXl+fW1rJlSx09elQbN25Us2bNJP0RCl0ul1q0aCFJWr16tdt58+bN04svvqhVq1YVCK0AAAAAUNIIiWcIDQ1VSkqKHnvsMblcLrVp00bHjh1Tenq6nE6n+vTpo5iYGL399ttavHixoqOjNXPmTK1fv17R0dF2nKioKC1evFjbt29XxYoVFRYWpnr16qljx47q16+fpkyZolOnTik5OVk9evRQZGSkJKlevXpu9WzYsEE+Pj4l8t2NAAAAAPBXyvxXYBRm5MiRGjZsmNLS0mywW7BggQ2BDzzwgLp3764777xTLVq00C+//OK2qyj98d7CunXrqnnz5qpcubLS09MlSe+8846uuuoqtW/fXp06dVKbNm00derUiz5HAAAAACiMwxhjvF0ESlZ2drbCwsJ024fp8g8p7+1yAAAAgDJjTkJDb5fgJj8bHDt2rMjfnsBOIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACw/bxcAz3m7Q6ycTqe3ywAAAABQirCTCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAMvP2wXAc3ov3Sz/kPLeLgMAAACl2JyEht4uARcZO4kAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQCAAAAACwCIkAAAAAAIuQeIa9e/fK4XAoIyPjbzVeVFSUxo8fXyI1AQAAAMDZEBIBAAAAABYhEQAAAABglcmQuGjRIrVp00bh4eGqWLGibr31Vu3ateus/bds2aJbb71VTqdToaGhatu2re3vcrk0YsQIVa9eXYGBgWrcuLEWLVpUYIzdu3fr+uuvV7ly5dSoUSOtXr3a7fkPP/xQDRo0UGBgoKKiojRmzJiSnTQAAAAAFEGZDIm//vqrHn/8cW3YsEGfffaZfHx81K1bN7lcrgJ9v//+e1133XUKDAzUsmXLtHHjRt133306ffq0JGnChAkaM2aMRo8erW+++UYJCQnq0qWLMjMz3cZ5+umnlZKSooyMDNWpU0c9e/a0Y2zcuFGJiYnq0aOHvv32Ww0fPlzDhg3T9OnTizSfkydPKjs72+0BAAAAAOfDz9sFeMPtt9/udvzWW2+pcuXK2rp1q8qXL+/23GuvvaawsDDNnj1b/v7+kqQ6derY50ePHq0nn3xSPXr0kCS9+OKLWr58ucaPH6/XXnvN9ktJSdEtt9wiSUpNTVWDBg20c+dOXXXVVRo7dqzat2+vYcOG2fG3bt2ql19+WUlJSX85n7S0NKWmphZ/IQAAAADgDGVyJzEzM1M9e/bUlVdeKafTqaioKElSVlZWgb4ZGRlq27atDYh/lp2drR9++EGtW7d2a2/durW2bdvm1tawYUP7c7Vq1SRJhw4dkiRt27at0DEyMzOVl5f3l/MZMmSIjh07Zh/79+//y3MAAAAAoDBlciexc+fOqlmzpt544w1FRkbK5XIpNjZWubm5BfoGBweXyDX/HDIdDockFfry1vMRGBiowMDAEhkLAAAAQNlW5nYSf/nlF23fvl1Dhw5V+/btVa9ePR05cuSs/Rs2bKgvvvhCp06dKvCc0+lUZGSk0tPT3drT09NVv379ItdUr169QseoU6eOfH19izwOAAAAAFyoMhcSL7vsMlWsWFFTp07Vzp07tWzZMj3++ONn7Z+cnKzs7Gz16NFDGzZsUGZmpmbOnKnt27dLkgYPHqwXX3xR7733nrZv365//etfysjI0KBBg4pc0xNPPKHPPvtMI0eO1I4dOzRjxgy9+uqrSklJueD5AgAAAEBxlLmXm/r4+Gj27NkaOHCgYmNjVbduXb3yyitq165dof0rVqyoZcuWafDgwYqPj5evr68aN25s30M4cOBAHTt2TE888YQOHTqk+vXr65NPPlFMTEyRa2ratKnef/99PfPMMxo5cqSqVaumESNGFOlDawAAAACgJDmMMcbbRaBkZWdnKywsTLd9mC7/kPJ/fQIAAABwFnMSGv51J/xt5WeDY8eOyel0FumcMvdyUwAAAADA2RESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAACWn7cLgOe83SFWTqfT22UAAAAAKEXYSQQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWH7eLgCe03vpZvmHlPd2GQAAAJesOQkNvV0CUOLYSQQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWF4PiUlJSerates5+7Rr106PPvpoiV53+PDhaty4cYmOCQAAAAClnZ+3C5gwYYKMMd4uAwAAAACgCwyJubm5CggIuKACwsLCLuj8sqYk1hwAAAAAzqZYLzdt166dkpOT9eijj6pSpUpKSEiQJG3evFk333yzypcvr6pVq+qee+7Rzz//bM/74IMPFBcXp+DgYFWsWFEdOnTQr7/+Kqngy01//fVX9e7dW+XLl1e1atU0ZsyYAnU4HA7NnTvXrS08PFzTp0+3x08++aTq1KmjcuXK6corr9SwYcN06tSpIs/1yJEj6tWrlypXrqzg4GDFxMRo2rRpkqQVK1bI4XDo6NGjtn9GRoYcDof27t1r29544w3VqFFD5cqVU7du3TR27FiFh4fb53ft2qXbbrtNVatWVfny5XX11Vdr6dKlbnVERUVp5MiR6t27t5xOp/r371/kOQAAAABAcRX7PYkzZsxQQECA0tPTNWXKFB09elQ33HCDmjRpog0bNmjRokX68ccflZiYKEk6cOCAevbsqfvuu0/btm3TihUr1L1797O+xHTw4MH6/PPPNW/ePH366adasWKFvvrqq2JPLDQ0VNOnT9fWrVs1YcIEvfHGGxo3blyRzx82bJi2bt2qhQsXatu2bZo8ebIqVapU5PPT09P14IMPatCgQcrIyNCNN96o559/3q1PTk6OOnXqpM8++0ybNm1Sx44d1blzZ2VlZbn1Gz16tBo1aqRNmzZp2LBhBa518uRJZWdnuz0AAAAA4HwU++WmMTExeumll+zxc889pyZNmmjUqFG27a233lKNGjW0Y8cO5eTk6PTp0+revbtq1qwpSYqLiyt07JycHP373//WrFmz1L59e0l/hNLq1asXt0wNHTrU/hwVFaWUlBTNnj1b//znP4t0flZWlpo0aaLmzZvbMYpj4sSJuvnmm5WSkiJJqlOnjlatWqX58+fbPo0aNVKjRo3s8ciRI/Xxxx/rk08+UXJysm2/4YYb9MQTT5z1WmlpaUpNTS1WfQAAAABQmGLvJDZr1szt+Ouvv9by5ctVvnx5+7jqqqsk/fFyykaNGql9+/aKi4vTHXfcoTfeeENHjhwpdOxdu3YpNzdXLVq0sG0VKlRQ3bp1i1um3nvvPbVu3VoREREqX768hg4dWmCH7lweeughzZ49W40bN9Y///lPrVq1qljX3759u6655hq3tjOPc3JylJKSonr16ik8PFzly5fXtm3bCtSZH1TPZsiQITp27Jh97N+/v1i1AgAAAEC+YofEkJAQt+OcnBx17txZGRkZbo/MzExdd9118vX11ZIlS7Rw4ULVr19fEydOVN26dbVnz57zLtrhcBR4ueqf32+4evVq9erVS506ddL8+fO1adMmPf3008rNzS3yNW6++Wbt27dPjz32mH744Qe1b9/e7gr6+PyxbH+uoTjvd8yXkpKijz/+WKNGjdIXX3yhjIwMxcXFFajzzDU/U2BgoJxOp9sDAAAAAM7HBX9PYtOmTbVlyxZFRUWpdu3abo/8cONwONS6dWulpqZq06ZNCggI0Mcff1xgrFq1asnf319r1661bUeOHNGOHTvc+lWuXFkHDhywx5mZmTpx4oQ9XrVqlWrWrKmnn35azZs3V0xMjPbt21fsuVWuXFl9+vTRrFmzNH78eE2dOtW2S3KrISMjw+3cunXrav369W5tZx6np6crKSlJ3bp1U1xcnCIiItw++AYAAAAALrYLDokDBgzQ4cOH1bNnT61fv167du3S4sWLde+99yovL09r167VqFGjtGHDBmVlZemjjz7STz/9pHr16hUYq3z58urbt68GDx6sZcuWafPmzUpKSrI7d/luuOEGvfrqq9q0aZM2bNigBx98UP7+/vb5mJgYZWVlafbs2dq1a5deeeWVQkPpuTzzzDOaN2+edu7cqS1btmj+/Pm25tq1a6tGjRoaPny4MjMztWDBggKfwvrII4/ov//9r8aOHavMzEy9/vrrWrhwoRwOh1udH330kTIyMvT111/rrrvuksvlKladAAAAAFCSLjgkRkZGKj09XXl5ebrpppsUFxenRx99VOHh4fLx8ZHT6dTKlSvVqVMn1alTR0OHDtWYMWN08803Fzreyy+/rLZt26pz587q0KGD2rRpU+B9kGPGjFGNGjXUtm1b3XXXXUpJSVG5cuXs8126dNFjjz2m5ORkNW7cWKtWrSr0U0HPJSAgQEOGDFHDhg3ty2Znz54tSfL399e7776r7777Tg0bNtSLL76o5557zu381q1ba8qUKRo7dqwaNWqkRYsW6bHHHlNQUJDtM3bsWF122WVq1aqVOnfurISEBDVt2rRYdQIAAABASXKYs30XBUpcv3799N133+mLL77w6HWys7MVFham2z5Ml39IeY9eCwAAoCybk9DQ2yUA55SfDY4dO1bkzy4p9ldgoOhGjx6tG2+8USEhIVq4cKFmzJihSZMmebssAAAAADgrQqIHrVu3Ti+99JKOHz+uK6+8Uq+88oruv/9+b5cFAAAAAGdFSPSg999/39slAAAAAECxXPAH1wAAAAAALh2ERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFh+3i4AnvN2h1g5nU5vlwEAAACgFGEnEQAAAABgERIBAAAAABYhEQAAAABgERIBAAAAABYhEQAAAABgERIBAAAAABYhEQAAAABgERIBAAAAABYhEQAAAABgERIBAAAAABYhEQAAAABgERIBAAAAABYhEQAAAABgERIBAAAAABYhEQAAAABgERIBAAAAAJaftwuA5/Reuln+IeW9XQYAALhEzElo6O0SAFwE7CQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCIgAAAADAIiQCAAAAACxCoiRjjPr3768KFSrI4XAoIyPD2yUBAAAAgFcQEiUtWrRI06dP1/z583XgwAHFxsZe8JhJSUnq2rVrgfbDhw+rV69ecjqdCg8PV9++fZWTk2Of//3335WUlKS4uDj5+fkVOgYAAAAAeAohUdKuXbtUrVo1tWrVShEREfLz8/PYtXr16qUtW7ZoyZIlmj9/vlauXKn+/fvb5/Py8hQcHKyBAweqQ4cOHqsDAAAAAApT5kNiUlKSHnnkEWVlZcnhcCgqKkoul0tpaWmKjo5WcHCwGjVqpA8++MCek5eXp759+9rn69atqwkTJtjnhw8frhkzZmjevHlyOBxyOBxasWKFtm3bpkWLFunNN99UixYt1KZNG02cOFGzZ8/WDz/8IEkKCQnR5MmT1a9fP0VERFz09QAAAABQtnluy6yUmDBhgmrVqqWpU6dq/fr18vX1VVpammbNmqUpU6YoJiZGK1eu1N13363KlSsrPj5eLpdL1atX15w5c1SxYkWtWrVK/fv3V7Vq1ZSYmKiUlBRt27ZN2dnZmjZtmiSpQoUKmjVrlsLDw9W8eXN7/Q4dOsjHx0dr165Vt27dzmsOJ0+e1MmTJ+1xdnb2hS0KAAAAgDKrzIfEsLAwhYaGytfXVxERETp58qRGjRqlpUuXqmXLlpKkK6+8Ul9++aVef/11xcfHy9/fX6mpqXaM6OhorV69Wu+//74SExNVvnx5BQcH6+TJk267gQcPHlSVKlXcru/n56cKFSro4MGD5z2HtLQ0t3oAAAAA4HyV+ZB4pp07d+rEiRO68cYb3dpzc3PVpEkTe/zaa6/prbfeUlZWln777Tfl5uaqcePGF7naPwwZMkSPP/64Pc7OzlaNGjW8UgsAAACA0o2QeIb8TxpdsGCBLr/8crfnAgMDJUmzZ89WSkqKxowZo5YtWyo0NFQvv/yy1q5de86xIyIidOjQIbe206dP6/Dhwxf0/sPAwEBbGwAAAABcCELiGerXr6/AwEBlZWUpPj6+0D7p6elq1aqVHn74Ydu2a9cutz4BAQHKy8tza2vZsqWOHj2qjRs3qlmzZpKkZcuWyeVyqUWLFiU8EwAAAAAoPkLiGUJDQ5WSkqLHHntMLpdLbdq00bFjx5Seni6n06k+ffooJiZGb7/9thYvXqzo6GjNnDlT69evV3R0tB0nKipKixcv1vbt21WxYkWFhYWpXr166tixo/r166cpU6bo1KlTSk5OVo8ePRQZGWnP3bp1q3Jzc3X48GEdP35cGRkZkuS1l7MCAAAAKDsIiYUYOXKkKleurLS0NO3evVvh4eFq2rSpnnrqKUnSAw88oE2bNunOO++Uw+FQz5499fDDD2vhwoV2jH79+mnFihVq3ry5cnJytHz5crVr107vvPOOkpOT1b59e/n4+Oj222/XK6+84nb9Tp06ad++ffY4/72QxpiLMHsAAAAAZZnDkDwuOdnZ2QoLC9NtH6bLP6S8t8sBAACXiDkJDb1dAoBiys8Gx44dk9PpLNI5Ph6uCQAAAABQihASAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAAAWIREAAAAAYBESAQAAAACWn7cLgOe83SFWTqfT22UAAAAAKEXYSQQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWIREAAAAAIBFSAQAAAAAWH7eLgCe03vpZvmHlPd2GQAAL5uT0NDbJQAAShF2EgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREv9G2rVrp0cffbRA+/Tp0xUeHn7R6wEAAABQ9hASAQAAAACWn7cLKEvatWun2NhYSdLMmTPl7++vhx56SCNGjJDD4fBydQAAAADATuJFN2PGDPn5+WndunWaMGGCxo4dqzfffPOCxjx58qSys7PdHgAAAABwPthJvMhq1KihcePGyeFwqG7duvr22281btw49evXT5I0adKkAqHx9OnTCgoKOuuYaWlpSk1N9WjdAAAAAMoGdhIvsmuvvdbtpaUtW7ZUZmam8vLyJEm9evVSRkaG22PEiBHnHHPIkCE6duyYfezfv9+jcwAAAABw6WIn8W8mLCxMtWvXdmurUqXKOc8JDAxUYGCgJ8sCAAAAUEawk3iRrV271u14zZo1iomJka+vr5cqAgAAAID/ISReZFlZWXr88ce1fft2vfvuu5o4caIGDRrk7bIAAAAAQBIvN73oevfurd9++03XXHONfH19NWjQIPXv39/bZQEAAACAJMlhjDHeLqKsaNeunRo3bqzx48d79DrZ2dkKCwvTbR+myz+kvEevBQD4+5uT0NDbJQAAvCQ/Gxw7dkxOp7NI5/ByUwAAAACARUgEAAAAAFi8J/EiWrFihbdLAAAAAIBzYicRAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGAREgEAAAAAFiERAAAAAGD5ebsAeM7bHWLldDq9XQYAAACAUoSdRAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACARUgEAAAAAFiERAAAAACA5eftAlDyjDGSpOzsbC9XAgAAAMCb8jNBfkYoCkLiJeiXX36RJNWoUcPLlQAAAAD4Ozh+/LjCwsKK1JeQeAmqUKGCJCkrK6vINwJKTnZ2tmrUqKH9+/fL6XR6u5wyhbX3Htbee1h772L9vYe19x7W3nvOZ+2NMTp+/LgiIyOLfB1C4iXIx+ePt5qGhYXxf1wvcjqdrL+XsPbew9p7D2vvXay/97D23sPae09x1764G0d8cA0AAAAAwCIkAgAAAAAsQuIlKDAwUM8++6wCAwO9XUqZxPp7D2vvPay997D23sX6ew9r7z2svfdcrLV3mOJ8FioAAAAA4JLGTiIAAAAAwCIkAgAAAAAsQiIAAAAAwCIkAgAAAAAsQmIp8dprrykqKkpBQUFq0aKF1q1bd87+c+bM0VVXXaWgoCDFxcXpv//9r9vzxhg988wzqlatmoKDg9WhQwdlZmZ6cgqlVkmu/alTp/Tkk08qLi5OISEhioyMVO/evfXDDz94ehqlUknf93/24IMPyuFwaPz48SVc9aXDE+u/bds2denSRWFhYQoJCdHVV1+trKwsT02h1Crptc/JyVFycrKqV6+u4OBg1a9fX1OmTPHkFEqt4qz9li1bdPvttysqKuqc/54U9/dZVpX02qelpenqq69WaGioqlSpoq5du2r79u0enEHp5ol7P98LL7wgh8OhRx99tGSLvkR4Yu2///573X333apYsaKCg4MVFxenDRs2FL0og7+92bNnm4CAAPPWW2+ZLVu2mH79+pnw8HDz448/Fto/PT3d+Pr6mpdeesls3brVDB061Pj7+5tvv/3W9nnhhRdMWFiYmTt3rvn6669Nly5dTHR0tPntt98u1rRKhZJe+6NHj5oOHTqY9957z3z33Xdm9erV5pprrjHNmjW7mNMqFTxx3+f76KOPTKNGjUxkZKQZN26ch2dSOnli/Xfu3GkqVKhgBg8ebL766iuzc+dOM2/evLOOWVZ5Yu379etnatWqZZYvX2727NljXn/9dePr62vmzZt3saZVKhR37detW2dSUlLMu+++ayIiIgr996S4Y5ZVnlj7hIQEM23aNLN582aTkZFhOnXqZK644gqTk5Pj4dmUPp5Y/z/3jYqKMg0bNjSDBg3yzARKMU+s/eHDh03NmjVNUlKSWbt2rdm9e7dZvHix2blzZ5HrIiSWAtdcc40ZMGCAPc7LyzORkZEmLS2t0P6JiYnmlltucWtr0aKFeeCBB4wxxrhcLhMREWFefvll+/zRo0dNYGCgeffddz0wg9KrpNe+MOvWrTOSzL59+0qm6EuEp9b+//7v/8zll19uNm/ebGrWrElIPAtPrP+dd95p7r77bs8UfAnxxNo3aNDAjBgxwq1P06ZNzdNPP12ClZd+xV37PzvbvycXMmZZ4om1P9OhQ4eMJPP5559fSKmXJE+t//Hjx01MTIxZsmSJiY+PJyQWwhNr/+STT5o2bdpcUF283PRvLjc3Vxs3blSHDh1sm4+Pjzp06KDVq1cXes7q1avd+ktSQkKC7b9nzx4dPHjQrU9YWJhatGhx1jHLIk+sfWGOHTsmh8Oh8PDwEqn7UuCptXe5XLrnnns0ePBgNWjQwDPFXwI8sf4ul0sLFixQnTp1lJCQoCpVqqhFixaaO3eux+ZRGnnq3m/VqpU++eQTff/99zLGaPny5dqxY4duuukmz0ykFDqftffGmJeii7VOx44dkyRVqFChxMa8FHhy/QcMGKBbbrmlwL9R+IOn1v6TTz5R8+bNdccdd6hKlSpq0qSJ3njjjWKNQUj8m/v555+Vl5enqlWrurVXrVpVBw8eLPScgwcPnrN//v8WZ8yyyBNrf6bff/9dTz75pHr27Cmn01kyhV8CPLX2L774ovz8/DRw4MCSL/oS4on1P3TokHJycvTCCy+oY8eO+vTTT9WtWzd1795dn3/+uWcmUgp56t6fOHGi6tevr+rVqysgIEAdO3bUa6+9puuuu67kJ1FKnc/ae2PMS9HFWCeXy6VHH31UrVu3VmxsbImMeanw1PrPnj1bX331ldLS0i60xEuWp9Z+9+7dmjx5smJiYrR48WI99NBDGjhwoGbMmFHkMfzO++oALsipU6eUmJgoY4wmT57s7XIueRs3btSECRP01VdfyeFweLucMsflckmSbrvtNj322GOSpMaNG2vVqlWaMmWK4uPjvVneJW/ixIlas2aNPvnkE9WsWVMrV67UgAEDFBkZyX/hR5kwYMAAbd68WV9++aW3SykT9u/fr0GDBmnJkiUKCgrydjlljsvlUvPmzTVq1ChJUpMmTbR582ZNmTJFffr0KdIY7CT+zVWqVEm+vr768ccf3dp//PFHRUREFHpORETEOfvn/29xxiyLPLH2+fID4r59+7RkyRJ2Ec/gibX/4osvdOjQIV1xxRXy8/OTn5+f9u3bpyeeeEJRUVEemUdp5Yn1r1Spkvz8/FS/fn23PvXq1ePTTf/EE2v/22+/6amnntLYsWPVuXNnNWzYUMnJybrzzjs1evRoz0ykFDqftffGmJciT69TcnKy5s+fr+XLl6t69eoXPN6lxhPrv3HjRh06dEhNmza1f3M///xzvfLKK/Lz81NeXl5JlF7qeerer1at2gX/vSUk/s0FBASoWbNm+uyzz2yby+XSZ599ppYtWxZ6TsuWLd36S9KSJUts/+joaEVERLj1yc7O1tq1a886ZlnkibWX/hcQMzMztXTpUlWsWNEzEyjFPLH299xzj7755htlZGTYR2RkpAYPHqzFixd7bjKlkCfWPyAgQFdffXWBj5/fsWOHatasWcIzKL08sfanTp3SqVOn5OPj/iff19fX7vDi/NbeG2Neijy1TsYYJScn6+OPP9ayZcsUHR1dEuVecjyx/u3bt9e3337r9je3efPm6tWrlzIyMuTr61tS5Zdqnrr3W7dufeF/by/oY29wUcyePdsEBgaa6dOnm61bt5r+/fub8PBwc/DgQWOMMffcc4/517/+Zfunp6cbPz8/M3r0aLNt2zbz7LPPFvoVGOHh4WbevHnmm2++MbfddhtfgVGIkl773Nxc06VLF1O9enWTkZFhDhw4YB8nT570yhz/rjxx35+JTzc9O0+s/0cffWT8/f3N1KlTTWZmppk4caLx9fU1X3zxxUWf39+ZJ9Y+Pj7eNGjQwCxfvtzs3r3bTJs2zQQFBZlJkyZd9Pn9nRV37U+ePGk2bdpkNm3aZKpVq2ZSUlLMpk2bTGZmZpHHxB88sfYPPfSQCQsLMytWrHD7e3vixImLPr+/O0+s/5n4dNPCeWLt161bZ/z8/Mzzzz9vMjMzzTvvvGPKlStnZs2aVeS6CImlxMSJE80VV1xhAgICzDXXXGPWrFljn4uPjzd9+vRx6//++++bOnXqmICAANOgQQOzYMECt+ddLpcZNmyYqVq1qgkMDDTt27c327dvvxhTKXVKcu337NljJBX6WL58+UWaUelR0vf9mQiJ5+aJ9f/3v/9tateubYKCgkyjRo3M3LlzPT2NUqmk1/7AgQMmKSnJREZGmqCgIFO3bl0zZswY43K5LsZ0SpXirP3Z/k2Pj48v8pj4n5Je+7P9vZ02bdrFm1Qp4ol7/88IiWfnibX/z3/+Y2JjY01gYKC56qqrzNSpU4tVk8MYY4q5gwkAAAAAuETxnkQAAAAAgEVIBAAAAABYhEQAAAAAgEVIBAAAAABYhEQAAAAAgEVIBAAAAABYhEQAAAAAgEVIBAAAAABYhEQAAAAAgEVIBABccpKSkuRwOAo8du7cWSLjT58+XeHh4SUy1vlKSkpS165dvVrDuezdu1cOh0MZGRneLgUAUEx+3i4AAABP6Nixo6ZNm+bWVrlyZS9Vc3anTp2Sv7+/t8soUbm5ud4uAQBwAdhJBABckgIDAxUREeH28PX1lSTNmzdPTZs2VVBQkK688kqlpqbq9OnT9tyxY8cqLi5OISEhqlGjhh5++GHl5ORIklasWKF7771Xx44dszuUw4cPlyQ5HA7NnTvXrY7w8HBNnz5d0v9219577z3Fx8crKChI77zzjiTpzTffVL169RQUFKSrrrpKkyZNKtZ827Vrp0ceeUSPPvqoLrvsMlWtWlVvvPGGfv31V917770KDQ1V7dq1tXDhQnvOihUr5HA4tGDBAjVs2FBBQUG69tprtXnzZrexP/zwQzVo0ECBgYGKiorSmDFj3J6PiorSyJEj1bt3bzmdTvXv31/R0dGSpCZNmsjhcKhdu3aSpPXr1+vGG29UpUqVFBYWpvj4eH311Vdu4zkcDr355pvq1q2bypUrp5iYGH3yySdufbZs2aJbb71VTqdToaGhatu2rXbt2mWfv9D1BICyjJAIAChTvvjiC/Xu3VuDBg3S1q1b9frrr2v69Ol6/vnnbR8fHx+98sor2rJli2bMmKFly5bpn//8pySpVatWGj9+vJxOpw4cOKADBw4oJSWlWDX861//0qBBg7Rt2zYlJCTonXfe0TPPPKPnn39e27Zt06hRozRs2DDNmDGjWOPOmDFDlSpV0rp16/TII4/ooYce0h133KFWrVrpq6++0k033aR77rlHJ06ccDtv8ODBGjNmjNavX6/KlSurc+fOOnXqlCRp48aNSkxMVI8ePfTtt99q+PDhGjZsmA2++UaPHq1GjRpp06ZNGjZsmNatWydJWrp0qQ4cOKCPPvpIknT8+HH16dNHX375pdasWaOYmBh16tRJx48fdxsvNTVViYmJ+uabb9SpUyf16tVLhw8fliR9//33uu666xQYGKhly5Zp48aNuu+++2zQL6n1BIAyywAAcInp06eP8fX1NSEhIfbxj3/8wxhjTPv27c2oUaPc+s+cOdNUq1btrOPNmTPHVKxY0R5PmzbNhIWFFegnyXz88cdubWFhYWbatGnGGGP27NljJJnx48e79alVq5b5f//v/7m1jRw50rRs2fKcc7ztttvscXx8vGnTpo09Pn36tAkJCTH33HOPbTtw4ICRZFavXm2MMWb58uVGkpk9e7bt88svv5jg4GDz3nvvGWOMueuuu8yNN97odu3Bgweb+vXr2+OaNWuarl27uvXJn+umTZvOOgdjjMnLyzOhoaHmP//5j22TZIYOHWqPc3JyjCSzcOFCY4wxQ4YMMdHR0SY3N7fQMc9nPQEA/8N7EgEAl6Trr79ekydPtschISGSpK+//lrp6eluO4d5eXn6/fffdeLECZUrV05Lly5VWlqavvvuO2VnZ+v06dNuz1+o5s2b259//fVX7dq1S3379lW/fv1s++nTpxUWFlascRs2bGh/9vX1VcWKFRUXF2fbqlatKkk6dOiQ23ktW7a0P1eoUEF169bVtm3bJEnbtm3Tbbfd5ta/devWGj9+vPLy8uxLeP88p3P58ccfNXToUK1YsUKHDh1SXl6eTpw4oaysrLPOJSQkRE6n09adkZGhtm3bFvpezpJcTwAoqwiJAIBLUkhIiGrXrl2gPScnR6mpqerevXuB54KCgrR3717deuuteuihh/T888+rQoUK+vLLL9W3b1/l5uaeMyQ6HA4ZY9za8l+2eWZtf65Hkt544w21aNHCrV9+ACuqM0OTw+Fwa3M4HJIkl8tVrHGL4s9zOpc+ffrol19+0YQJE1SzZk0FBgaqZcuWBT7sprC55NcdHBx81vFLcj0BoKwiJAIAypSmTZtq+/bthQZI6Y/34LlcLo0ZM0Y+Pn+8df/999936xMQEKC8vLwC51auXFkHDhywx5mZmQXe/3emqlWrKjIyUrt371avXr2KO50SsWbNGl1xxRWSpCNHjmjHjh2qV6+eJKlevXpKT09365+enq46deqcM3QFBARIUoF1Sk9P16RJk9SpUydJ0v79+/Xzzz8Xq96GDRtqxowZhX4y7N9hPQGgtCMkAgDKlGeeeUa33nqrrrjiCv3jH/+Qj4+Pvv76a23evFnPPfecateurVOnTmnixInq3Lmz0tPTNWXKFLcxoqKilJOTo88++0yNGjVSuXLlVK5cOd1www169dVX1bJlS+Xl5enJJ58s0tdbpKamauDAgQoLC1PHjh118uRJbdiwQUeOHNHjjz/uqaWwRowYoYoVK6pq1ap6+umnValSJfsdjE888YSuvvpqjRw5UnfeeadWr16tV1999S8/LbRKlSoKDg7WokWLVL16dQUFBSksLEwxMTGaOXOmmjdvruzsbA0ePPicO4OFSU5O1sSJE9WjRw8NGTJEYWFhWrNmja655hrVrVvX6+sJAKUdn24KAChTEhISNH/+fH366ae6+uqrde2112rcuHGqWbOmJKlRo0YaO3asXnzxRcXGxuqdd95RWlqa2xitWrXSgw8+qDvvvFOVK1fWSy+9JEkaM2aMatSoobZt2+quu+5SSkpKkd7DeP/99+vNN9/UtGnTFBcXp/j4eE2fPt1+jYSnvfDCCxo0aJCaNWumgwcP6j//+Y/dCWzatKnef/99zZ49W7GxsXrmmWc0YsQIJSUlnXNMPz8/vfLKK3r99dcVGRlp39f473//W0eOHFHTpk11zz33aODAgapSpUqx6q1YsaKWLVumnJwcxcfHq1mzZnrjjTdsIPf2egJAaecwZ755AgAAlAkrVqzQ9ddfryNHjig8PNzb5QAA/ibYSQQAAAAAWIREAAAAAIDFy00BAAAAABY7iQAAAAAAi5AIAAAAALAIiQAAAAAAi5AIAAAAALAIiQAAAAAAi5AIAAAAALAIiQAAAAAAi5AIAAAAALD+P9kvykpf35g+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline.fit(X, y)\n",
    "\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "\n",
    "# Select the last five elements\n",
    "top_features_idx = sorted_idx[-5:]\n",
    "\n",
    "# Plotting only the top five features\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X.columns[top_features_idx], rf.feature_importances_[top_features_idx], color = '#54b8d1')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 5 Features Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practice we wanted to check the performance of neural networks in our data. We wanted to chekc some of the configurations and it's performance. We tested many architectures of the network of neurons, and the configuration below gave the best results. Then we wanted to check which activation fuction would be the most powerful. To do that we created a loop that train and validate the model outcome for different confgurations f learning rate and activation functions. It turned out that learing rate = 0.01 adn activation fuction 'relu' gave the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\filsz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>activation_func</th>\n",
       "      <th>mse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.211825</td>\n",
       "      <td>0.868018</td>\n",
       "      <td>15.087844</td>\n",
       "      <td>1.100829</td>\n",
       "      <td>0.080330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1.317773</td>\n",
       "      <td>0.914930</td>\n",
       "      <td>15.903255</td>\n",
       "      <td>1.147943</td>\n",
       "      <td>-0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1.319503</td>\n",
       "      <td>0.916068</td>\n",
       "      <td>15.923037</td>\n",
       "      <td>1.148696</td>\n",
       "      <td>-0.001389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1.323625</td>\n",
       "      <td>0.918186</td>\n",
       "      <td>15.959859</td>\n",
       "      <td>1.150489</td>\n",
       "      <td>-0.004517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.100</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.362759</td>\n",
       "      <td>0.933385</td>\n",
       "      <td>16.224039</td>\n",
       "      <td>1.167373</td>\n",
       "      <td>-0.034216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.100</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1.377482</td>\n",
       "      <td>0.938429</td>\n",
       "      <td>16.311721</td>\n",
       "      <td>1.173662</td>\n",
       "      <td>-0.045390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>tanh</td>\n",
       "      <td>1.409723</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>16.504602</td>\n",
       "      <td>1.187318</td>\n",
       "      <td>-0.069858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>1.432669</td>\n",
       "      <td>0.957252</td>\n",
       "      <td>16.638893</td>\n",
       "      <td>1.196941</td>\n",
       "      <td>-0.087272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>relu</td>\n",
       "      <td>1.543144</td>\n",
       "      <td>0.988366</td>\n",
       "      <td>17.179721</td>\n",
       "      <td>1.242234</td>\n",
       "      <td>-0.171113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lr activation_func       mse       mae       mape      rmse        r2\n",
       "3  0.010            relu  1.211825  0.868018  15.087844  1.100829  0.080330\n",
       "4  0.010            tanh  1.317773  0.914930  15.903255  1.147943 -0.000076\n",
       "7  0.100            tanh  1.319503  0.916068  15.923037  1.148696 -0.001389\n",
       "5  0.010         sigmoid  1.323625  0.918186  15.959859  1.150489 -0.004517\n",
       "6  0.100            relu  1.362759  0.933385  16.224039  1.167373 -0.034216\n",
       "8  0.100         sigmoid  1.377482  0.938429  16.311721  1.173662 -0.045390\n",
       "1  0.001            tanh  1.409723  0.949526  16.504602  1.187318 -0.069858\n",
       "2  0.001         sigmoid  1.432669  0.957252  16.638893  1.196941 -0.087272\n",
       "0  0.001            relu  1.543144  0.988366  17.179721  1.242234 -0.171113"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['quality', 'id', 'density', 'feat02', 'feat03', 'feat05', 'feat06', 'feat08', 'feat09', 'feat10', 'fixed acidity'], axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "cols_to_scale = X.columns.drop(['chlorides', 'residual sugar', 'sulphates'])\n",
    "\n",
    "cols_to_encode = ['chlorides', 'residual sugar', 'sulphates']\n",
    "\n",
    "numeric_transformer = MinMaxScaler()\n",
    "categoric_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, cols_to_scale),\n",
    "        ('cat', categoric_transformer, cols_to_encode)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "columns = cols_to_scale.tolist() + preprocessor.named_transformers_['cat'].get_feature_names_out(cols_to_encode).tolist()\n",
    "X_train_scaled = pd.DataFrame(X_train, columns=columns)\n",
    "\n",
    "X_test = preprocessor.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test, columns=columns)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "activation_functions = ['relu', 'tanh', 'sigmoid']\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for activation_func in activation_functions:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation=activation_func))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(64, activation=activation_func))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(64, activation=activation_func))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(64, activation=activation_func))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(32, activation=activation_func))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(16, activation=activation_func))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_scaled, y_train, epochs=300, batch_size=64, validation_split=0.2, verbose=0)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        predictions = model.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, predictions)\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        mape = mean_absolute_error(y_test, predictions) / abs(y_test).mean() * 100\n",
    "        rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "\n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'activation_func': activation_func,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'mape': mape,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values('mape', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project aimed to apply a comprehensive set of machine learning techniques learned in class, including boosting, bagging, stacking, and neural networks. It involved a systematic approach to machine learning, covering essential steps such as initial data preprocessing, data cleaning, exploratory data analysis, feature engineering, and preprocessing. To ensure robustness, the project employed advanced methodologies like avoiding data leakage through the use of pipelines.\n",
    "\n",
    "The modeling phase included the implementation of various ensemble methods like boosting, bagging, and stacking, along with the integration of neural networks to explore their capabilities. The project followed best practices in machine learning, incorporating hyperparameter tuning through randomized search to optimize model performance.\n",
    "\n",
    "Results of the modeeling were discussed during the presentation in class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
